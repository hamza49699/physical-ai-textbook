"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4719],{2854:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-3-isaac","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"Overview","source":"@site/docs/module-3-isaac.md","sourceDirName":".","slug":"/module-3-isaac","permalink":"/physical-ai-textbook/docs/module-3-isaac","draft":false,"unlisted":false,"editUrl":"https://github.com/hamza49699/physical-ai-textbook/tree/main/docs/module-3-isaac.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Introduction: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/physical-ai-textbook/docs/module-3-intro"},"next":{"title":"Module 4 Introduction: Vision-Language-Action (VLA)","permalink":"/physical-ai-textbook/docs/module-4-intro"}}');var o=a(4848),i=a(8453);const t={sidebar_position:3},s="Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"3.1 NVIDIA Isaac Ecosystem",id:"31-nvidia-isaac-ecosystem",level:2},{value:"Three Core Components",id:"three-core-components",level:3},{value:"Installation",id:"installation",level:3},{value:"3.2 Isaac Sim: Photorealistic Simulation",id:"32-isaac-sim-photorealistic-simulation",level:2},{value:"Creating a Simulation Environment",id:"creating-a-simulation-environment",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:3},{value:"3.3 Isaac ROS: Hardware-Accelerated VSLAM",id:"33-isaac-ros-hardware-accelerated-vslam",level:2},{value:"Visual SLAM (Simultaneous Localization and Mapping)",id:"visual-slam-simultaneous-localization-and-mapping",level:3},{value:"Using VSLAM Data for Navigation",id:"using-vslam-data-for-navigation",level:3},{value:"3.4 Nav2: Path Planning for Humanoids",id:"34-nav2-path-planning-for-humanoids",level:2},{value:"Launch Nav2",id:"launch-nav2",level:3},{value:"Path Planning Example",id:"path-planning-example",level:3},{value:"Waypoint Following",id:"waypoint-following",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(n){const e={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"module-3-the-ai-robot-brain-nvidia-isaac",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"NVIDIA Isaac provides the perception and AI stack for autonomous robotics. It combines photorealistic simulation (Isaac Sim), hardware-accelerated vision (Isaac ROS), and path planning (Nav2)."}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"What you'll learn:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Photorealistic simulation with Isaac Sim"}),"\n",(0,o.jsx)(e.li,{children:"Synthetic data generation for training"}),"\n",(0,o.jsx)(e.li,{children:"Hardware-accelerated VSLAM (Visual SLAM)"}),"\n",(0,o.jsx)(e.li,{children:"Path planning for humanoid navigation"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"31-nvidia-isaac-ecosystem",children:"3.1 NVIDIA Isaac Ecosystem"}),"\n",(0,o.jsx)(e.h3,{id:"three-core-components",children:"Three Core Components"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502         Isaac Sim (Simulation)          \u2502\r\n\u2502  - Photorealistic rendering             \u2502\r\n\u2502  - Physics simulation                   \u2502\r\n\u2502  - Synthetic data generation            \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                    \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502       Isaac ROS (Perception)            \u2502\r\n\u2502  - GPU-accelerated VSLAM                \u2502\r\n\u2502  - Real-time perception                 \u2502\r\n\u2502  - Hardware integration                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n                    \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502        Nav2 (Path Planning)             \u2502\r\n\u2502  - Motion planning                      \u2502\r\n\u2502  - Navigation                           \u2502\r\n\u2502  - Obstacle avoidance                   \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Install Isaac Sim (Omniverse platform required)\r\n# Download: https://www.nvidia.com/en-us/omniverse/download/\r\n\r\n# Install Isaac ROS\r\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\r\ncd isaac_ros_common\r\n./scripts/run_dev.sh\r\n\r\n# Install Nav2\r\nsudo apt install ros-humble-navigation2 ros-humble-nav2-bringup\n"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Resources:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.nvidia.com/isaac/",children:"NVIDIA Isaac Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"Isaac ROS GitHub"})}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"32-isaac-sim-photorealistic-simulation",children:"3.2 Isaac Sim: Photorealistic Simulation"}),"\n",(0,o.jsx)(e.h3,{id:"creating-a-simulation-environment",children:"Creating a Simulation Environment"}),"\n",(0,o.jsx)(e.p,{children:"Isaac Sim runs on NVIDIA Omniverse and provides tools to build synthetic worlds."}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Python API Example:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import omni.graph.core as og\r\nfrom omni.isaac.kit import SimulationApp\r\n\r\n# Initialize Isaac Sim\r\nsimulation_app = SimulationApp({"headless": False})\r\nsimulation_context = simulation_app.context\r\n\r\nfrom omni.isaac.core import World\r\n\r\n# Create world\r\nworld = World(stage_units_in_meters=1.0)\r\n\r\n# Add robot to scene\r\nfrom omni.isaac.manipulators import SingleArm\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\n\r\n# Load humanoid robot\r\nadd_reference_to_stage(\r\n    usd_path="omniverse://nvidia/Assets/Isaac/4.0/Isaac/Robots/Humanoids/G1/g1.usd",\r\n    prim_path="/World/Robot"\r\n)\r\n\r\n# Play simulation\r\nsimulation_context.play()\r\n\r\n# Step simulation at 60 Hz\r\nfor frame in range(1000):\r\n    simulation_context.step(render=True)\r\n\r\nsimulation_app.close()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,o.jsx)(e.p,{children:"Generate labeled datasets for training perception models:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"from omni.isaac.synthetic_utils import SyntheticDataHelper\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass DatasetGenerator:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.helper = SyntheticDataHelper()\r\n        \r\n    def generate_dataset(self, num_frames=1000):\r\n        \"\"\"Generate RGB, depth, and segmentation data\"\"\"\r\n        \r\n        dataset = {\r\n            'rgb': [],\r\n            'depth': [],\r\n            'segmentation': []\r\n        }\r\n        \r\n        for frame in range(num_frames):\r\n            # Randomize lighting and camera angle\r\n            self.randomize_environment()\r\n            \r\n            # Capture RGB image\r\n            rgb = self.helper.get_rgb()\r\n            dataset['rgb'].append(rgb)\r\n            \r\n            # Capture depth\r\n            depth = self.helper.get_depth()\r\n            dataset['depth'].append(depth)\r\n            \r\n            # Capture semantic segmentation\r\n            seg = self.helper.get_segmentation()\r\n            dataset['segmentation'].append(seg)\r\n            \r\n            self.world.step(render=False)\r\n        \r\n        return dataset\r\n    \r\n    def randomize_environment(self):\r\n        \"\"\"Randomize lighting, textures, and object positions\"\"\"\r\n        # Randomize lighting\r\n        light_intensity = np.random.uniform(0.5, 2.0)\r\n        \r\n        # Randomize camera position\r\n        camera_x = np.random.uniform(-2, 2)\r\n        camera_y = np.random.uniform(-2, 2)\r\n        \r\n        self.world.update_camera_position([camera_x, camera_y, 1.5])\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"33-isaac-ros-hardware-accelerated-vslam",children:"3.3 Isaac ROS: Hardware-Accelerated VSLAM"}),"\n",(0,o.jsx)(e.h3,{id:"visual-slam-simultaneous-localization-and-mapping",children:"Visual SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,o.jsx)(e.p,{children:"VSLAM uses camera images to build a map while tracking the robot's position."}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Launch Isaac ROS VSLAM:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Start ROS 2 bridge from Isaac Sim\r\npython3 isaac_sim_ros2_bridge.py\r\n\r\n# Terminal 2: Launch VSLAM\r\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\r\n\r\n# Terminal 3: Visualize\r\nrviz2 -d vslam_config.rviz\n"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"VSLAM Configuration (vslam.yaml):"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'vslam_node:\r\n  ros__parameters:\r\n    # Camera parameters\r\n    camera_topic: "/camera/image_rect"\r\n    camera_info_topic: "/camera/camera_info"\r\n    \r\n    # Detector and descriptor settings\r\n    detector_type: "sift"  # or "orb", "akaze"\r\n    descriptor_type: "sift"\r\n    max_keypoints: 500\r\n    \r\n    # Matching parameters\r\n    match_ratio: 0.7\r\n    max_matches: 100\r\n    \r\n    # Pose estimation\r\n    min_tracked_features: 20\r\n    \r\n    # Output\r\n    pose_topic: "/visual_odometry/pose"\r\n    map_topic: "/visual_odometry/map"\n'})}),"\n",(0,o.jsx)(e.h3,{id:"using-vslam-data-for-navigation",children:"Using VSLAM Data for Navigation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom nav_msgs.msg import Odometry\r\nfrom geometry_msgs.msg import Twist\r\n\r\nclass VSLAMNavigator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'vslam_navigator\')\r\n        \r\n        # Subscribe to VSLAM odometry\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/visual_odometry/pose\',\r\n            self.odometry_callback,\r\n            10\r\n        )\r\n        \r\n        # Publish movement commands\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        \r\n        self.robot_pose = None\r\n        \r\n    def odometry_callback(self, msg):\r\n        """Update robot pose from VSLAM"""\r\n        self.robot_pose = msg.pose.pose\r\n        x = self.robot_pose.position.x\r\n        y = self.robot_pose.position.y\r\n        self.get_logger().info(f"Robot position: ({x:.2f}, {y:.2f})")\r\n        \r\n    def move_to_target(self, target_x, target_y):\r\n        """Navigate to target using VSLAM feedback"""\r\n        if self.robot_pose is None:\r\n            self.get_logger().warning("Waiting for VSLAM data...")\r\n            return\r\n        \r\n        # Calculate distance to target\r\n        current_x = self.robot_pose.position.x\r\n        current_y = self.robot_pose.position.y\r\n        \r\n        dx = target_x - current_x\r\n        dy = target_y - current_y\r\n        distance = (dx**2 + dy**2)**0.5\r\n        \r\n        if distance < 0.1:\r\n            self.get_logger().info("Target reached!")\r\n            return\r\n        \r\n        # Send movement command\r\n        twist = Twist()\r\n        twist.linear.x = 0.3  # Move forward\r\n        self.cmd_pub.publish(twist)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"34-nav2-path-planning-for-humanoids",children:"3.4 Nav2: Path Planning for Humanoids"}),"\n",(0,o.jsx)(e.p,{children:"Nav2 (Navigation 2) provides autonomous navigation with obstacle avoidance."}),"\n",(0,o.jsx)(e.h3,{id:"launch-nav2",children:"Launch Nav2"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"ros2 launch nav2_bringup bringup_launch.py use_sim_time:=true use_rviz:=true\n"})}),"\n",(0,o.jsx)(e.h3,{id:"path-planning-example",children:"Path Planning Example"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom nav2_simple_commander.robot_navigator import BasicNavigator\r\nfrom geometry_msgs.msg import PoseStamped\r\nimport tf_transformations\r\n\r\nclass HumanoidNavigator(Node):\r\n    def __init__(self):\r\n        super().__init__(\'humanoid_navigator\')\r\n        self.navigator = BasicNavigator()\r\n        \r\n    def navigate_to_pose(self, x, y, theta):\r\n        """Navigate to a target pose"""\r\n        \r\n        # Create goal pose\r\n        goal_pose = PoseStamped()\r\n        goal_pose.header.frame_id = \'map\'\r\n        goal_pose.header.stamp = self.get_clock().now().to_msg()\r\n        \r\n        goal_pose.pose.position.x = x\r\n        goal_pose.pose.position.y = y\r\n        \r\n        # Convert angle to quaternion\r\n        q = tf_transformations.quaternion_from_euler(0, 0, theta)\r\n        goal_pose.pose.orientation.x = q[0]\r\n        goal_pose.pose.orientation.y = q[1]\r\n        goal_pose.pose.orientation.z = q[2]\r\n        goal_pose.pose.orientation.w = q[3]\r\n        \r\n        # Navigate\r\n        self.navigator.goToPose(goal_pose)\r\n        self.get_logger().info(f"Navigating to ({x}, {y})")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    navigator = HumanoidNavigator()\r\n    \r\n    # Navigation sequence\r\n    navigator.navigate_to_pose(1.0, 0.0, 0.0)  # Move forward\r\n    navigator.navigate_to_pose(1.0, 1.0, 1.57)  # Turn and move\r\n    \r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"waypoint-following",children:"Waypoint Following"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from nav2_simple_commander.robot_navigator import BasicNavigator\r\n\r\nclass WaypointNavigator:\r\n    def __init__(self):\r\n        self.navigator = BasicNavigator()\r\n        \r\n    def follow_waypoints(self, waypoints):\r\n        """Follow a sequence of waypoints"""\r\n        for i, (x, y, theta) in enumerate(waypoints):\r\n            goal_pose = PoseStamped()\r\n            goal_pose.header.frame_id = \'map\'\r\n            goal_pose.pose.position.x = x\r\n            goal_pose.pose.position.y = y\r\n            \r\n            q = tf_transformations.quaternion_from_euler(0, 0, theta)\r\n            goal_pose.pose.orientation.x = q[0]\r\n            goal_pose.pose.orientation.y = q[1]\r\n            goal_pose.pose.orientation.z = q[2]\r\n            goal_pose.pose.orientation.w = q[3]\r\n            \r\n            print(f"Going to waypoint {i+1}: ({x}, {y})")\r\n            self.navigator.goToPose(goal_pose)\r\n\r\n# Define waypoints (x, y, angle)\r\nwaypoints = [\r\n    (0.0, 0.0, 0.0),    # Start\r\n    (2.0, 0.0, 0.0),    # Move forward\r\n    (2.0, 2.0, 1.57),   # Turn right and move\r\n    (0.0, 2.0, 3.14),   # Turn right and move\r\n    (0.0, 0.0, 0.0),    # Return to start\r\n]\r\n\r\nnavigator = WaypointNavigator()\r\nnavigator.follow_waypoints(waypoints)\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"Isaac Sim"})," = Photorealistic simulation with synthetic data generation",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"VSLAM"})," = GPU-accelerated visual localization and mapping",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"Nav2"})," = Production-grade autonomous navigation",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"Integration"})," = Seamless workflow from simulation to real robots"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next:"})," Module 4 - Vision-Language-Action (VLA) and Capstone"]})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>t,x:()=>s});var r=a(6540);const o={},i=r.createContext(o);function t(n){const e=r.useContext(i);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),r.createElement(i.Provider,{value:e},n.children)}}}]);
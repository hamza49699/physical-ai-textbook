"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6071],{6514:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"Overview","source":"@site/docs/module-4-vla.md","sourceDirName":".","slug":"/module-4-vla","permalink":"/physical-ai-textbook/docs/module-4-vla","draft":false,"unlisted":false,"editUrl":"https://github.com/hamza49699/physical-ai-textbook/tree/main/docs/module-4-vla.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 Introduction: Vision-Language-Action (VLA)","permalink":"/physical-ai-textbook/docs/module-4-intro"},"next":{"title":"Module 5 Introduction: Advanced Humanoid Control (Motion & Learning)","permalink":"/physical-ai-textbook/docs/module-5-intro"}}');var o=r(4848),i=r(8453);const a={sidebar_position:4},s="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"4.1 Voice-to-Action Pipeline",id:"41-voice-to-action-pipeline",level:2},{value:"Architecture",id:"architecture",level:3},{value:"4.2 Speech Recognition with OpenAI Whisper",id:"42-speech-recognition-with-openai-whisper",level:2},{value:"Installation",id:"installation",level:3},{value:"Basic Voice Input",id:"basic-voice-input",level:3},{value:"Real-time Streaming with OpenAI API",id:"real-time-streaming-with-openai-api",level:3},{value:"4.3 Cognitive Planning with LLMs",id:"43-cognitive-planning-with-llms",level:2},{value:"Using GPT-4 for Action Planning",id:"using-gpt-4-for-action-planning",level:3},{value:"Multi-step Reasoning with Chain-of-Thought",id:"multi-step-reasoning-with-chain-of-thought",level:3},{value:"4.4 Grounding Language in Robot Actions",id:"44-grounding-language-in-robot-actions",level:2},{value:"Semantic Understanding for Objects",id:"semantic-understanding-for-objects",level:3},{value:"4.5 Capstone Project: The Autonomous Humanoid",id:"45-capstone-project-the-autonomous-humanoid",level:2},{value:"Full System Integration",id:"full-system-integration",level:3},{value:"Complete Example Commands",id:"complete-example-commands",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2}];function p(n){const e={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"Vision-Language-Action (VLA) is where robotics meets large language models. A robot receives voice commands, understands them with AI, plans a sequence of actions, and executes them in the real world."}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"What you'll learn:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Converting voice to text with OpenAI Whisper"}),"\n",(0,o.jsx)(e.li,{children:"Cognitive planning with LLMs (GPT-4)"}),"\n",(0,o.jsx)(e.li,{children:"Grounding language in robot actions"}),"\n",(0,o.jsx)(e.li,{children:"The capstone: Autonomous Humanoid Robot"}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"41-voice-to-action-pipeline",children:"4.1 Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(e.h3,{id:"architecture",children:"Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Voice Input \u2502 (microphone)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  Whisper (STT)  \u2502 (speech-to-text)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  NLP Understanding      \u2502 (parse intent)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  LLM Planning (GPT-4)   \u2502 (generate action plan)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502  ROS 2 Execution        \u2502 (execute actions)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n       \u2502\r\n       \u25bc\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Robot Action \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"42-speech-recognition-with-openai-whisper",children:"4.2 Speech Recognition with OpenAI Whisper"}),"\n",(0,o.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper pyaudio\n"})}),"\n",(0,o.jsx)(e.h3,{id:"basic-voice-input",children:"Basic Voice Input"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import whisper\r\nimport pyaudio\r\nimport numpy as np\r\n\r\nclass VoiceInput:\r\n    def __init__(self):\r\n        self.model = whisper.load_model("base")  # or "tiny", "small", "medium", "large"\r\n        \r\n    def record_audio(self, duration=5):\r\n        """Record audio from microphone"""\r\n        CHUNK = 1024\r\n        FORMAT = pyaudio.paFloat32\r\n        CHANNELS = 1\r\n        RATE = 16000\r\n        \r\n        p = pyaudio.PyAudio()\r\n        \r\n        stream = p.open(\r\n            format=FORMAT,\r\n            channels=CHANNELS,\r\n            rate=RATE,\r\n            input=True,\r\n            frames_per_buffer=CHUNK\r\n        )\r\n        \r\n        print("Recording...")\r\n        frames = []\r\n        for _ in range(0, int(RATE / CHUNK * duration)):\r\n            data = stream.read(CHUNK)\r\n            frames.append(np.frombuffer(data, dtype=np.float32))\r\n        \r\n        stream.stop_stream()\r\n        stream.close()\r\n        p.terminate()\r\n        \r\n        audio = np.concatenate(frames)\r\n        return audio\r\n    \r\n    def transcribe(self, audio):\r\n        """Convert audio to text"""\r\n        result = self.model.transcribe(\r\n            audio,\r\n            language="en",\r\n            task="transcribe"\r\n        )\r\n        return result["text"]\r\n    \r\n    def listen_and_transcribe(self, duration=5):\r\n        """Record and transcribe voice"""\r\n        audio = self.record_audio(duration)\r\n        text = self.transcribe(audio)\r\n        return text\r\n\r\n# Usage\r\nvoice_input = VoiceInput()\r\ncommand = voice_input.listen_and_transcribe()\r\nprint(f"Heard: {command}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"real-time-streaming-with-openai-api",children:"Real-time Streaming with OpenAI API"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport pyaudio\r\n\r\nopenai.api_key = "your-api-key"\r\n\r\nclass StreamingVoiceInput:\r\n    def __init__(self):\r\n        self.buffer = b""\r\n        \r\n    def stream_and_transcribe(self):\r\n        """Stream audio to Whisper API"""\r\n        \r\n        CHUNK = 1024\r\n        FORMAT = pyaudio.paFloat32\r\n        CHANNELS = 1\r\n        RATE = 16000\r\n        \r\n        p = pyaudio.PyAudio()\r\n        stream = p.open(\r\n            format=FORMAT,\r\n            channels=CHANNELS,\r\n            rate=RATE,\r\n            input=True,\r\n            frames_per_buffer=CHUNK\r\n        )\r\n        \r\n        print("Listening...")\r\n        \r\n        # Record until silence\r\n        silence_duration = 0\r\n        max_silence = 3\r\n        \r\n        while silence_duration < max_silence:\r\n            data = stream.read(CHUNK)\r\n            self.buffer += data\r\n            \r\n            # Simple silence detection\r\n            audio_data = np.frombuffer(data, dtype=np.float32)\r\n            if np.abs(audio_data).mean() < 0.01:\r\n                silence_duration += CHUNK / RATE\r\n            else:\r\n                silence_duration = 0\r\n        \r\n        stream.stop_stream()\r\n        stream.close()\r\n        p.terminate()\r\n        \r\n        # Send to Whisper\r\n        with open("temp_audio.wav", "wb") as f:\r\n            f.write(self.buffer)\r\n        \r\n        with open("temp_audio.wav", "rb") as f:\r\n            transcript = openai.Audio.transcribe("whisper-1", f)\r\n        \r\n        return transcript["text"]\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"43-cognitive-planning-with-llms",children:"4.3 Cognitive Planning with LLMs"}),"\n",(0,o.jsx)(e.h3,{id:"using-gpt-4-for-action-planning",children:"Using GPT-4 for Action Planning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import openai\r\nimport json\r\n\r\nopenai.api_key = "your-api-key"\r\n\r\nclass CognitivePlanner:\r\n    def __init__(self):\r\n        self.system_prompt = """\r\nYou are a robot planning system. Convert natural language commands into a sequence of robot actions.\r\nRespond ONLY with valid JSON in this format:\r\n{\r\n    "actions": [\r\n        {"action": "move_forward", "distance": 1.0, "duration": 5},\r\n        {"action": "rotate", "angle": 90, "direction": "left"},\r\n        {"action": "grasp", "object": "cup"}\r\n    ],\r\n    "reasoning": "explanation of the plan"\r\n}\r\n\r\nAvailable actions:\r\n- move_forward: Move forward (distance in meters)\r\n- move_backward: Move backward (distance in meters)\r\n- rotate: Rotate (angle in degrees, direction: "left" or "right")\r\n- grasp: Grasp object (object name)\r\n- release: Release held object\r\n- look_for: Search for object (object name)\r\n- identify: Identify what the robot sees\r\n- speak: Say something (text)\r\n"""\r\n    \r\n    def plan_action(self, command):\r\n        """Convert natural language to action plan"""\r\n        \r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {"role": "system", "content": self.system_prompt},\r\n                {"role": "user", "content": f"Command: {command}"}\r\n            ],\r\n            temperature=0.7,\r\n            max_tokens=500\r\n        )\r\n        \r\n        plan_text = response[\'choices\'][0][\'message\'][\'content\']\r\n        \r\n        try:\r\n            plan = json.loads(plan_text)\r\n            return plan\r\n        except json.JSONDecodeError:\r\n            return {"actions": [], "error": "Failed to parse plan"}\r\n    \r\n    def execute_plan(self, plan, robot_controller):\r\n        """Execute action plan on robot"""\r\n        for action in plan.get("actions", []):\r\n            action_type = action.get("action")\r\n            \r\n            if action_type == "move_forward":\r\n                robot_controller.move_forward(action.get("distance", 1.0))\r\n            elif action_type == "move_backward":\r\n                robot_controller.move_backward(action.get("distance", 1.0))\r\n            elif action_type == "rotate":\r\n                robot_controller.rotate(\r\n                    action.get("angle", 90),\r\n                    action.get("direction", "left")\r\n                )\r\n            elif action_type == "grasp":\r\n                robot_controller.grasp(action.get("object", ""))\r\n            elif action_type == "release":\r\n                robot_controller.release()\r\n            elif action_type == "speak":\r\n                robot_controller.speak(action.get("text", ""))\r\n\r\n# Example usage\r\nplanner = CognitivePlanner()\r\ncommand = "Pick up the cup and place it on the table"\r\nplan = planner.plan_action(command)\r\nprint(f"Action Plan: {json.dumps(plan, indent=2)}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"multi-step-reasoning-with-chain-of-thought",children:"Multi-step Reasoning with Chain-of-Thought"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class AdvancedPlanner:\r\n    def __init__(self):\r\n        pass\r\n    \r\n    def plan_with_reasoning(self, command):\r\n        """Use chain-of-thought for complex tasks"""\r\n        \r\n        # Step 1: Understand the task\r\n        understand_response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": f"Understand this robot task: {command}\\n\\nBreak it down into subtasks."\r\n                }\r\n            ]\r\n        )\r\n        \r\n        subtasks = understand_response[\'choices\'][0][\'message\'][\'content\']\r\n        print(f"Subtasks:\\n{subtasks}")\r\n        \r\n        # Step 2: Plan each subtask\r\n        plan_response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": f"For these subtasks:\\n{subtasks}\\n\\nGenerate robot actions in JSON format."\r\n                }\r\n            ]\r\n        )\r\n        \r\n        actions = plan_response[\'choices\'][0][\'message\'][\'content\']\r\n        return actions\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"44-grounding-language-in-robot-actions",children:"4.4 Grounding Language in Robot Actions"}),"\n",(0,o.jsx)(e.h3,{id:"semantic-understanding-for-objects",children:"Semantic Understanding for Objects"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class SemanticGrounding:\r\n    def __init__(self):\r\n        self.object_database = {\r\n            "cup": {"grasp_type": "cylindrical", "force": 5},\r\n            "ball": {"grasp_type": "spherical", "force": 3},\r\n            "book": {"grasp_type": "flat", "force": 7},\r\n            "bottle": {"grasp_type": "cylindrical", "force": 4}\r\n        }\r\n    \r\n    def get_grasp_strategy(self, object_name):\r\n        """Get how to grasp an object"""\r\n        if object_name in self.object_database:\r\n            return self.object_database[object_name]\r\n        else:\r\n            # Query GPT for unknown objects\r\n            response = openai.ChatCompletion.create(\r\n                model="gpt-4",\r\n                messages=[\r\n                    {\r\n                        "role": "user",\r\n                        "content": f"How should a robot grasp a {object_name}? (grasp_type and force)"\r\n                    }\r\n                ]\r\n            )\r\n            return response[\'choices\'][0][\'message\'][\'content\']\r\n    \r\n    def plan_manipulation(self, action_description):\r\n        """Convert description to manipulation command"""\r\n        # Example: "carefully pick up the fragile vase"\r\n        \r\n        # Extract object and modifiers\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": f"Parse this action: \'{action_description}\'\\nExtract: object_name, care_level, grip_strength"\r\n                }\r\n            ]\r\n        )\r\n        \r\n        return response[\'choices\'][0][\'message\'][\'content\']\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"45-capstone-project-the-autonomous-humanoid",children:"4.5 Capstone Project: The Autonomous Humanoid"}),"\n",(0,o.jsx)(e.h3,{id:"full-system-integration",children:"Full System Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport json\r\nimport openai\r\n\r\nclass AutonomousHumanoid(Node):\r\n    def __init__(self):\r\n        super().__init__(\'autonomous_humanoid\')\r\n        \r\n        # Initialize components\r\n        self.voice_input = VoiceInput()\r\n        self.planner = CognitivePlanner()\r\n        \r\n        # ROS publishers/subscribers\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.grasp_pub = self.create_publisher(JointCommand, \'/gripper_cmd\', 10)\r\n        \r\n        # Vision integration\r\n        self.vision_sub = self.create_subscription(Image, \'/camera/image\', self.process_image, 10)\r\n        \r\n        self.main_loop()\r\n    \r\n    def main_loop(self):\r\n        """Main execution loop"""\r\n        \r\n        print("\ud83e\udd16 Autonomous Humanoid Robot Ready")\r\n        print("Voice control enabled. Speak a command...")\r\n        \r\n        while True:\r\n            try:\r\n                # 1. Listen to voice command\r\n                print("\\n\ud83d\udce2 Listening...")\r\n                command = self.voice_input.listen_and_transcribe(duration=3)\r\n                print(f"\u2705 Heard: \'{command}\'")\r\n                \r\n                # 2. Plan actions\r\n                print("\\n\ud83e\udde0 Planning actions...")\r\n                plan = self.planner.plan_action(command)\r\n                print(f"\ud83d\udccb Plan: {json.dumps(plan[\'actions\'], indent=2)}")\r\n                \r\n                # 3. Execute plan\r\n                print("\\n\ud83e\udd16 Executing plan...")\r\n                self.execute_action_plan(plan[\'actions\'])\r\n                \r\n                print("\u2728 Task complete!")\r\n                \r\n            except KeyboardInterrupt:\r\n                print("\\n\ud83d\uded1 Shutting down...")\r\n                break\r\n            except Exception as e:\r\n                print(f"\u274c Error: {e}")\r\n    \r\n    def execute_action_plan(self, actions):\r\n        """Execute a sequence of actions"""\r\n        for action in actions:\r\n            action_type = action.get("action")\r\n            \r\n            if action_type == "move_forward":\r\n                self.move_forward(action.get("distance", 1.0))\r\n            elif action_type == "rotate":\r\n                self.rotate(action.get("angle", 90))\r\n            elif action_type == "look_for":\r\n                self.look_for_object(action.get("object", ""))\r\n            elif action_type == "grasp":\r\n                self.grasp_object(action.get("object", ""))\r\n            elif action_type == "release":\r\n                self.release_object()\r\n            elif action_type == "identify":\r\n                self.identify_object()\r\n    \r\n    def move_forward(self, distance):\r\n        """Move forward (in meters)"""\r\n        print(f"  \u27a1\ufe0f  Moving forward {distance}m...")\r\n        twist = Twist()\r\n        twist.linear.x = 0.3\r\n        for _ in range(int(distance * 10)):\r\n            self.cmd_pub.publish(twist)\r\n            self.get_clock().sleep_for(0.1)\r\n        twist.linear.x = 0.0\r\n        self.cmd_pub.publish(twist)\r\n    \r\n    def rotate(self, angle):\r\n        """Rotate (in degrees)"""\r\n        print(f"  \ud83d\udd04 Rotating {angle} degrees...")\r\n        twist = Twist()\r\n        twist.angular.z = 0.5\r\n        for _ in range(int(abs(angle) / 3)):\r\n            self.cmd_pub.publish(twist)\r\n            self.get_clock().sleep_for(0.1)\r\n        twist.angular.z = 0.0\r\n        self.cmd_pub.publish(twist)\r\n    \r\n    def grasp_object(self, object_name):\r\n        """Grasp an object"""\r\n        print(f"  \u270b Grasping {object_name}...")\r\n        msg = JointCommand()\r\n        msg.name = ["gripper_left", "gripper_right"]\r\n        msg.position = [0.8, 0.8]\r\n        self.grasp_pub.publish(msg)\r\n    \r\n    def release_object(self):\r\n        """Release held object"""\r\n        print(f"  \ud83d\udc50 Releasing object...")\r\n        msg = JointCommand()\r\n        msg.name = ["gripper_left", "gripper_right"]\r\n        msg.position = [0.0, 0.0]\r\n        self.grasp_pub.publish(msg)\r\n    \r\n    def look_for_object(self, object_name):\r\n        """Search for an object"""\r\n        print(f"  \ud83d\udc40 Looking for {object_name}...")\r\n        # Rotate to search\r\n        for angle in [-45, -20, 0, 20, 45]:\r\n            twist = Twist()\r\n            twist.angular.z = float(angle) / 10\r\n            self.cmd_pub.publish(twist)\r\n    \r\n    def identify_object(self):\r\n        """Identify what the robot sees"""\r\n        print(f"  \ud83d\udd0d Identifying objects...")\r\n        # Use vision model\r\n        pass\r\n    \r\n    def process_image(self, msg):\r\n        """Process camera images"""\r\n        # TODO: Implement vision processing\r\n        pass\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    robot = AutonomousHumanoid()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"complete-example-commands",children:"Complete Example Commands"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'# The robot can now understand and execute:\r\n\r\n"Move forward and look for a cup"\r\n# \u2192 Moves forward, scans environment, identifies cup\r\n\r\n"Pick up the cup and place it on the table"\r\n# \u2192 Locates cup, grasps it carefully, navigates to table, releases\r\n\r\n"Clean up the room"\r\n# \u2192 Plans entire cleanup sequence with Nav2 pathfinding\r\n\r\n"What do you see?"\r\n# \u2192 Uses vision to describe the environment\r\n\r\n"Follow me"\r\n# \u2192 Tracks person using VSLAM, follows at safe distance\n'})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(e.p,{children:["\u2705 ",(0,o.jsx)(e.strong,{children:"Whisper"})," = Robust speech recognition",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"GPT-4"})," = Intelligent task planning",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"Semantic Grounding"})," = Understanding objects and actions",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"Integration"})," = End-to-end autonomous robotics",(0,o.jsx)(e.br,{}),"\n","\u2705 ",(0,o.jsx)(e.strong,{children:"Scalability"})," = From simulation to real robots"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/",children:"OpenAI API Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Concepts.html",children:"ROS 2 Best Practices"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://discourse.ros.org/",children:"Robotics Community"})}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"\ud83c\udf89 Congratulations! You've mastered the complete Physical AI stack!"})})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const o={},i=t.createContext(o);function a(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(i.Provider,{value:e},n.children)}}}]);